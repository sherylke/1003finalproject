{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm, linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "data = pd.read_csv('./twitter-airline-sentiment/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_raw = data['airline_sentiment']\n",
    "# labeling data\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_raw)\n",
    "le.classes_\n",
    "y = le.transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "tweet = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get word counts\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "class OneVsAllClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    One-vs-all classifier\n",
    "    We assume that the classes will be the integers 0,..,(n_classes-1).\n",
    "    We assume that the estimator provided to the class, after fitting, has a \"decision_function\" that \n",
    "    returns the score for the positive class.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_classes):      \n",
    "        \"\"\"\n",
    "        Constructed with the number of classes and an estimator (e.g. an\n",
    "        SVM estimator from sklearn)\n",
    "        @param estimator : binary base classifier used\n",
    "        @param n_classes : number of classes\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes \n",
    "        self.estimators = [clone(estimator) for _ in range(n_classes)]\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This should fit one classifier for each class.\n",
    "        self.estimators[i] should be fit on class i vs rest\n",
    "        @param X: array-like, shape = [n_samples,n_features], input data\n",
    "        @param y: array-like, shape = [n_samples,] class labels\n",
    "        @return returns self\n",
    "        \"\"\"\n",
    "        #Your code goes here\n",
    "        # design a binary label of y for each estimator\n",
    "        for i in range(self.n_classes):\n",
    "            y_bi = np.copy(y)\n",
    "            y_bi[y_bi!=i]=-1\n",
    "            self.estimators[i].fit(X,y_bi)\n",
    "        self.fitted = True  \n",
    "        return self   \n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Returns the score of each input for each class. Assumes\n",
    "        that the given estimator also implements the decision_function method (which sklearn SVMs do), \n",
    "        and that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_features] input data\n",
    "        @return array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        if not hasattr(self.estimators[0], \"decision_function\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have a decision_function attribute.\")\n",
    "        \n",
    "        #Replace the following return statement with your code\n",
    "        score = np.zeros([X.shape[0],self.n_classes])\n",
    "        for i in range(self.n_classes):\n",
    "            score[:,i] = self.estimators[i].decision_function(X)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples,n_features] input data\n",
    "        @returns array-like, shape = [n_samples,] the predicted classes for each input\n",
    "        \"\"\"\n",
    "        #Replace the following return statement with your code\n",
    "        score = self.decision_function(X)\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction[i] = np.argmax(score[i])\n",
    "        return prediction\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Here we test the OneVsAllClassifier\n",
    "svm_estimator = svm.LinearSVC(loss='hinge', fit_intercept=False, C=0.15)\n",
    "clf_onevsall = OneVsAllClassifier(svm_estimator, n_classes=3)\n",
    "clf_onevsall.fit(X_train,y_train)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Coeffs %d\"%i)\n",
    "    print(clf_onevsall.estimators[i].coef_) #Will fail if you haven't implemented fit yet\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf_onevsall.predict(X_valid)[i])))\n",
    "print(1-np.sum(loss)/len(y_valid))\n",
    "    \n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_valid, clf_onevsall.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100,kernel='linear')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf.predict(X_valid)[i])))\n",
    "print(1-np.sum(loss)/len(y_valid))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ovo = OneVsOneClassifier(svm.LinearSVC(C=0.25))\n",
    "ovo.fit(X_train, y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(ovo.predict(X_valid)[i])))\n",
    "print(1-np.sum(loss)/len(y_valid))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, ovo.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boosting\n",
    "clf_gbm = GradientBoostingClassifier()\n",
    "clf_gbm.fit(X_train,y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf_gbm.predict(X_valid.toarray())[i])))\n",
    "print(1-np.sum(loss)/len(y_valid))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, clf_gbm.predict(X_valid.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest\n",
    "clf_rfc = RandomForestClassifier(n_estimators=50,criterion='entropy')\n",
    "clf_rfc = clf_rfc.fit(X_train,y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf_rfc.predict(X_valid)[i])))\n",
    "print(1-np.sum(loss)/len(y_valid))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, clf_rfc.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf=linear_model.LogisticRegression(C=0.4)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf.predict(X_valid)[i])))\n",
    "print(np.sum(loss))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_train, tweet_valid, ty_train, ty_valid = train_test_split(tweet, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ty_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,max_df = 0.8,sublinear_tf=True,use_idf=True)\n",
    "tX_train = vectorizer.fit_transform(tweet_train)\n",
    "tX_valid = vectorizer.transform(tweet_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svm_estimator = svm.LinearSVC(loss='hinge', fit_intercept=False, C=0.85)\n",
    "clf_onevsall = OneVsAllClassifier(svm_estimator, n_classes=3)\n",
    "clf_onevsall.fit(tX_train,ty_train)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Coeffs %d\"%i)\n",
    "    print(clf_onevsall.estimators[i].coef_) #Will fail if you haven't implemented fit yet\n",
    "\n",
    "loss = []\n",
    "for i in range(len(ty_valid)):\n",
    "    loss.append(int(ty_valid[i]!=int(clf_onevsall.predict(tX_valid)[i])))\n",
    "print(1-np.sum(loss)/len(ty_valid))\n",
    "    \n",
    "metrics.confusion_matrix(ty_valid, clf_onevsall.predict(tX_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100,kernel='linear')\n",
    "clf.fit(tX_train,ty_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(ty_valid)):\n",
    "    loss.append(int(ty_valid[i]!=int(clf.predict(tX_valid)[i])))\n",
    "print(1-np.sum(loss)/len(ty_valid))\n",
    "\n",
    "metrics.confusion_matrix(ty_valid, clf.predict(tX_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ovo = OneVsOneClassifier(svm.LinearSVC(C=0.25))\n",
    "ovo.fit(tX_train, ty_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(ty_valid)):\n",
    "    loss.append(int(ty_valid[i]!=int(ovo.predict(tX_valid)[i])))\n",
    "print(1-np.sum(loss)/len(ty_valid))\n",
    "\n",
    "metrics.confusion_matrix(ty_valid, ovo.predict(tX_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_gbm = GradientBoostingClassifier()\n",
    "clf_gbm.fit(tX_train,ty_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(ty_valid)):\n",
    "    loss.append(int(ty_valid[i]!=int(clf_gbm.predict(tX_valid.toarray())[i])))\n",
    "print(1-np.sum(loss)/len(ty_valid))\n",
    "\n",
    "metrics.confusion_matrix(ty_valid, clf_gbm.predict(tX_valid.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_rfc = RandomForestClassifier(n_estimators=10,criterion='entropy')\n",
    "clf_rfc = clf_rfc.fit(tX_train,ty_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(ty_valid)):\n",
    "    loss.append(int(ty_valid[i]!=int(clf_rfc.predict(tX_valid)[i])))\n",
    "print(1-np.sum(loss)/len(ty_valid))\n",
    "\n",
    "metrics.confusion_matrix(ty_valid, clf_rfc.predict(tX_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf=linear_model.LogisticRegression(C=0.4)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "loss = []\n",
    "for i in range(len(y_valid)):\n",
    "    loss.append(int(y_valid[i]!=int(clf.predict(X_valid)[i])))\n",
    "print(np.sum(loss))\n",
    "\n",
    "metrics.confusion_matrix(y_valid, clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    \n",
    "    psi = np.zeros([num_samples, num_inFeatures * num_classes])\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_inFeatures):\n",
    "            psi[i,y * num_inFeatures + j] = X[i,j]\n",
    "    return psi\n",
    "\n",
    "\n",
    "\n",
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=3, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,X,y, eta = 0.1, T = 10000):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        \n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        #np.random.seed(2)\n",
    "        random_ind = np.array(range(num_samples))\n",
    "        np.random.shuffle(random_ind)\n",
    "        t = 1\n",
    "        w = np.zeros(self.num_outFeatures)\n",
    "        while t <= T:\n",
    "            for ind in random_ind:\n",
    "                y_max = np.zeros(self.num_classes)\n",
    "                for y_i in range(self.num_classes):\n",
    "                    #ind = np.where(X==x)[0][0]\n",
    "                    y_max[y_i] = self.Delta(y[ind],y_i) + np.dot(self.Psi(X,y_i)[ind] - self.Psi(X,y[ind])[ind],w)\n",
    "                y_hat = np.argmax(y_max)\n",
    "        \n",
    "                subgrad = 2 * self.lam * w + (self.Psi(X,y_hat)[ind] - self.Psi(X,y[ind])[ind])\n",
    "                w = w - subgrad * eta\n",
    "                t = t + 1\n",
    "        return w\n",
    "        \n",
    "    def fit(self,X,y,eta=0.1,T=10000):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = self.subgradient(X,y,eta = 0.1, T = 100)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        score = np.zeros([X.shape[0],self.num_classes])\n",
    "        for j in range(self.num_classes):\n",
    "            score[:,j] = np.dot(self.Psi(X,j),self.coef_)\n",
    "        return score\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "\n",
    "        score = self.decision_function(X)\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            prediction[i] = np.argmax(score[i])\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the following code tests the MulticlassSVM and sgd\n",
    "#will fail if MulticlassSVM is not implemented yet\n",
    "est = MulticlassSVM(45153,lam=1)\n",
    "est.fit(X_train,y_train,eta=0.01) # choose a smaller eta of 0.01\n",
    "print(\"w:\")\n",
    "print(est.coef_)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
